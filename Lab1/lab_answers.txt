#####1#####
(c)
Q: Gore definirana funkcija pogreÅ¡ke ğ¸(â„|D) i funkcija srednje kvadratne
pogreÅ¡ke nisu posve identiÄne. U Äemu je razlika? Koja je "realnija"?

A: Gore definirana funkcija pogreÅ¡ke ima mnogo veÄ‡e vrijednosti, dok druga
funkcija radi srednju vrijednost svih greÅ¡aka, stoga je ona realnija jer radi
prosjek greÅ¡aka cijelog modela.


(d)
Uvjerite se da za primjere iz D teÅ¾ine ğ° ne moÅ¾emo naÄ‡i rjeÅ¡avanjem sustava
ğ°=ğš½^âˆ’1ğ², veÄ‡ da nam doista treba pseudoinverz.
Q: ZaÅ¡to je to sluÄaj? Bi li se problem mogao rijeÅ¡iti preslikavanjem primjera
 u viÅ¡u dimenziju? Ako da, bi li to uvijek funkcioniralo, neovisno o skupu
  primjera D? PokaÅ¾ite na primjeru.

A: Nije moguÄ‡e naÄ‡i inverz matrice koja nije kvadratna, a FI Äesto nije
kvadratna matrica. MoguÄ‡e je dobitni kvadratnu matricu preslikavanjem primjera
u viÅ¡u dimenziju, ali onda je i potrebno imati jednak broj primjera i
preslikanih broja znaÄajki, te kada bi N bio puno veÄ‡i m + 1 bi trebao biti isto
velik. Zbog toga se radi pseudoinverz koji sadrÅ¾i i Gramovu matricu za koju
postoji inverz.



#####3#####
(a)
Q: Koji model ima najmanju pogreÅ¡ku uÄenja i zaÅ¡to? (predict se radi na X_train)

A: Najmanju pogreÅ¡ku uÄenja ima model sa najveÄ‡im stupnjem polinoma, zato Å¡to
on ima najveÄ‡i kapacitet, ali je zapravo prenauÄen i nebi dobro radio na
neviÄ‘enim podacima.

(b)
Q: Je li rezultat u skladu s oÄekivanjima? Koji biste model odabrali i zaÅ¡to?

A: U skladu je s oÄekivanjima, zato Å¡to modeli sa velikim stupnjevima polinoma
radi skoro savrÅ¡eno na viÄ‘enim podacima, ali je presloÅ¾en i loÅ¡e generalizira.
Dok kod manje sloÅ¾enih modela moÅ¾e biti problem to Å¡to nema dovoljan kapacitet,
ali bolje generalizira.

Q: Pokrenite iscrtavanje viÅ¡e puta. U Äemu je problem? Bi li problem bio
jednako izraÅ¾en kad bismo imali viÅ¡e primjera? ZaÅ¡to?

A: Koji problem?!

(c)
Q: Jesu li rezultati oÄekivani? ObrazloÅ¾ite.

A: Jesu heh. Zato Å¡to sa veÄ‡om Sigmom pogreÅ¡ke su veÄ‡e, takoÄ‘er za veÄ‡i broj
primjera pogreÅ¡ke su veÄ‡e.


#####4#####
(a)
Q: Kojih je dimenzija matrica koju treba invertirati?

A: Matrica je kvadratna, tj konkretno 4x4

Q: Po Äemu se razlikuju dobivene teÅ¾ine i je li ta razlika oÄekivana?
ObrazloÅ¾ite.

A: TeÅ¾ine se razlikuju po iznosima, odnosno Å¡to je veÄ‡a lambda, to je manji
iznos teÅ¾ina. Neke od teÅ¾ina su se smanjile gotovo na 0. Smisao je kazniti
hipoteze sa velikim teÅ¾inama.


#####5#####
(a)
Q: Jesu li rezultati oÄekivani? ObrazloÅ¾ite.

A: Jesu. Manji stupanj polinoma tj d=2 predstavlja gotovo ravne linije tj. ima
manju sloÅ¾enost, dok d=10 predstavlja sloÅ¾enije krivulje.

(b)
Q: Kojoj strani na grafikonu odgovara podruÄje prenauÄenosti, a kojoj
podnauÄenosti? ZaÅ¡to?

A: Nezz


Q: Koju biste vrijednosti za ğœ† izabrali na temelju ovih grafikona i zaÅ¡to?

A: nezz


#####6#####
(a)
Q: Objasnite oblik obiju krivulja. HoÄ‡e li krivulja za â€–ğ°â€–2 doseÄ‡i nulu? ZaÅ¡to?
Je li to problem? ZaÅ¡to?

A: Norme teÅ¾ina sa veÄ‡om lambdom sve su manje i manje, ali krivulja ||W||2 ne
dosegne nulu Å¡to je problem jer ne stvara rijetke modele.


Q: Za ğœ†=100, koliki je postotak teÅ¾ina modela jednak nuli, odnosno koliko je
model rijedak?

A: Niti jedna teÅ¾ina nije 0, stoga model uopÄ‡e nije rijedak.

(b)
Uglavnom ovdje koristimo L1 regularizaciju (LASSO), pa dobivamo teÅ¾ine
postavljene na 0, tj dobivamo rijetki model.



#####7#####
(b)
Q: GledajuÄ‡i grafikone iz podzadatka (a), koja znaÄajka bi trebala imati veÄ‡u
magnitudu, odnosno vaÅ¾nost pri predikciji prosjeka na studiju? Odgovaraju li
teÅ¾ine VaÅ¡oj intuiciji? Objasnite.

A: Exam score bi trebao imati veÄ‡u magnitudu. TeÅ¾ine odgovaraju tome, buduÄ‡i da
je teÅ¾ina na exam score veÄ‡a.



#####8#####
(a)
Q: Usporedite iznose teÅ¾ina s onima koje ste dobili u zadatku 7b. Å to se
dogodilo?

A: Duplicirane teÅ¾ine za ocjene iz srednje Å¡kole, sada su puno manje, gotovo 0.

(b)
Q: Kako regularizacija utjeÄe na stabilnost teÅ¾ina?

A: Å to je veÄ‡a regularizacija, standardna devijacija je manja, tj. stabilnost
je veÄ‡a.

Q: Jesu li koeficijenti jednakih magnituda kao u prethodnom pokusu?
Objasnite zaÅ¡to.

A: Pa ono. Za lambdu=0.01 su gotovo isti, jer je stupanj regularizacije manji,
a za lambdu=1000 su dosta manji.


(c)
Q: Kako regularizacija utjeÄe na kondicijski broj matrice ğš½âŠºğš½+ğœ†ğˆ?

A: Å to je veÄ‡i stupanj regularizacije, to je kondicijski broj manji. A Å¡to je
kondicijski broj veÄ‡i, rjeÅ¡enje je nestabilno tj. to dovodi do prenauÄenosti.
