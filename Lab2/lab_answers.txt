#####1#####
(a)
Q: Kako bi bila definirana granica izmeÄ‘u klasa ako bismo koristili oznake klasa
 âˆ’1 i 1 umjesto 0 i 1?

A: Granica bi bila definirana h(x)>=0.0

(b)
Q: ZaÅ¡to model ne ostvaruje potpunu toÄnost iako su podatci linearno odvojivi?

A: Zato Å¡to linearno diskriminativni modeli, zbog svoje fukcije gubitka, koja je
kvadratna, kaÅ¾njavaju i dobro oznaÄene primjere. Takvi modeli su nerobusni na
vrijednosti koje odskaÄu.

(c)
Q: OÄito je zaÅ¡to model nije u moguÄ‡nosti postiÄ‡i potpunu toÄnost na ovom skupu
podataka. MeÄ‘utim, smatrate li da je problem u modelu ili u podacima?
Argumentirajte svoj stav.

A: Model nije mogao odvojiti podatke koji su linearno neodvojivi. Problem s
podacima je to Å¡to su linearno neodvojivi, a problem u modelu je Å¡to nije
dovoljno sloÅ¾en.


#####2#####
Q: Alternativna shema jest ona zvana jedan-naspram-jedan(engl, one-vs-one, OVO).
Koja je prednost sheme OVR nad shemom OVO? A obratno?

A: OVO koristi (K povrh 2) binarnih klasifikatora, dok OVR koristi K binarnih
klasifikatora. Problem kod OVR-a je to Å¡to, je moguÄ‡e da doÄ‘e do disbalansa
broja klasa, odnosno pozitivnih klasa Ä‡e generalno biti puno manje od zbroja
ostalih klasa.


#####3#####
(a)
Q: ZaÅ¡to je sigmoidalna funkcija prikladan izbor za aktivacijsku funkciju
poopÄ‡enoga linearnog modela?

A: Prikladan je izbor zato Å¡to ima probabilistiÄku interpretaciju. TakoÄ‘er
derivabilna je.


Q: Kakav utjecaj ima faktor ğ›¼ na oblik sigmoide? Å to to znaÄi za model
logistiÄke regresije (tj. kako izlaz modela ovisi o normi vektora teÅ¾ina ğ°)?

A: Å to je veÄ‡i faktor alpha to je sigmoida strmija, tj brÅ¾e prelazi iz 0 u 1.
Å to je veÄ‡a norma vektora teÅ¾ina w, to je sigmoida strmija, a to ujedno znaÄi
da za neki mali pomak h(x,w) Ä‡e model pojedini primjerak pomaknuti ili u 1 ili
u 0.

(c)
Q: Koji kriterij zaustavljanja je aktiviran?

A: Broj iteracija je dosegnuo maksimalan broj. Dok za neke druge stope uÄenja
kriterij zaustavljanja bude zbog smanjenja unkarsne pogreÅ¡e izmeÄ‘u iteracija.
U principu za veÄ‡e stope uÄenja model brÅ¾e konvergira, no mogu Ä‡e je da i ne
naÄ‘e minimum zbog divergencije, dok za male stope uÄenja presporo doÄ‘e u
minimum.
________________________________________________________________________________

Q: ZaÅ¡to dobivena pogreÅ¡ka unakrsne entropije nije jednaka nuli?

A: Zbog svojstva sigmoide. Sigmoida tek u + beskonaÄnosti postaje 1, a 0 tek u
- beskonaÄnosti. Stoga niti jedan primjerak nije 100% toÄan. I uvijek postoji
barem neka greÅ¡ka.
________________________________________________________________________________

Q: Kako biste utvrdili da je optimizacijski postupak doista pronaÅ¡ao hipotezu
koja minimizira pogreÅ¡ku uÄenja? O Äemu to ovisi?

A: Ili kada bi unakrsna pogreÅ¡ka prestala padati, ili kada bi pala na neku
malu vrijednost. Tj. kada bi optimizacija konvergirala.
________________________________________________________________________________

Q: Na koji naÄin biste preinaÄili kÃ´d ako biste htjeli da se optimizacija izvodi
stohastiÄkim gradijentnim spustom (online learning)?

A: Za svaki primjerak bi se teÅ¾ine mijenjale, a ne tek nakon prolaska kroz sve
primjerke. Tj. nakon izraÄuna funkcije pogreÅ¡ke za jedan primjerak teÅ¾ine bi
se aÅ¾urirale.


(d)
Q: ZaÅ¡to je pogreÅ¡ka unakrsne entropije veÄ‡a od pogreÅ¡ke klasifikacije? Je li to
uvijek sluÄaj kod logistiÄke regresije i zaÅ¡to?

A: VeÄ‡a je zato Å¡to sigmoida nikada neÄ‡e prijerak staviti toÄno u 1, odnosno u 0,
pa Ä‡e uvijek postojati neka greÅ¡ka. Dok kod pogreÅ¡ke klasifikacije ili je
primjerak toÄno klasificiran ili nije.
________________________________________________________________________________


Q: Koju stopu uÄenja ğœ‚ biste odabrali i zaÅ¡to?

A: VaLda najveÄ‡u jer najbrÅ¾e doÄ‘e do minimuma.




#####4#####
(a)
Q: ZaÅ¡to se rezultat razlikuje od onog koji je dobio model klasifikacije
linearnom regresijom iz prvog zadatka?

A: Razlog leÅ¾i u funkciji pogreÅ¡ke. U linearnoj regresiji se kaÅ¾njavaju ispravno
klasificirani primjerci, dok u logistiÄkoj regresiji, jako ispravni primjerci
se gotovo uopÄ‡e ne kaÅ¾njavaju.

(c)
Q: Usporedite grafikone za sluÄaj linearno odvojivih i linearno neodvojivih
primjera te komentirajte razliku.

A: Ne razliku se previÅ¡e, osim Å¡to teÅ¾ine postaju manje, za linearno neodvojive
primjerke.


#####5#####
(a)
Q: Jesu li izgledi krivulja oÄekivani i zaÅ¡to?

A: Jesu. Za najveÄ‡u regularizaciju unakrsna greÅ¡ka je najveÄ‡a, buduÄ‡i da se
provodi na primjerima za uÄenje, pa model nije toliko sloÅ¾en, dok za malu
regularizaciju pogreÅ¡ka se smanjuje jer je model sloÅ¾eniji i bolje se
prilagoÄ‘uje primjerima za uÄenje.
Å to se tiÄe teÅ¾ina, regularizirane teÅ¾ine su manje od neregulariziranih.
________________________________________________________________________________

Q: Koju biste vrijednost za ğ›¼ odabrali i zaÅ¡to?

A: Pa nezz, treba vidjet koji aplha daje najmanju greÅ¡ku na neviÄ‘enim podacima.



#####6#####
Q: Koji biste stupanj polinoma upotrijebili i zaÅ¡to? Je li taj odabir povezan s
odabirom regularizacijskog faktora ğ›¼? ZaÅ¡to?

A: I jedan i drugi stupanj polinoma dobro klasificiraju primjerke, ali bolje
je imati manji stupanj polinoma ako i dalje dobro klasificira. 









k
