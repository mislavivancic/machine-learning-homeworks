################## 1 ##################
a)
Q: Koliko iznosi Å¡irina margine i zaÅ¡to?
A: Å irina margine iznosi pribliÅ¾no 1. To predstavlja maksimalnu udaljenost
od najbliÅ¾ih primjera lijevo i desno (potpornih vektora). Udaljenost se 
raÄuna kao d = h(x)/||w||

Q: Koji primjeri su potporni vektori i zaÅ¡to?
A: To su primjeri koji sami Äine granicu, odnosno oni primjeri koji se nalaze
na samoj margini. Zbog ograniÄenja da su najbliÅ¾i primjeri udaljeni za 1. 
Tako da zbog uvijeta alfa*(y*h(x)-1)=0,u predikciji h(x)=suma(alfa*y_i*xT*x_i) + w0
Za ostale primjere vrijedi da su udaljeniji od 1, pa je njihov alfa=0 kako bi
se zadovoljio uvijet.


c)
Q: Kako strÅ¡eÄ‡a vrijednost utjeÄe na SVM?
A: Ne utjeÄe nikako, buduÄ‡i da samo primjeri na margini utjeÄu na model.
Ne utjeÄu zato Å¡to primjeri iza margine (ondosno ispravno klasificirani), 
rezultiraju gubitkom 0. 

Q: Kako se linearan SVM nosi s linearno neodvojivim skupom podataka?
A: Pa gubi na toÄnosti, buduÄ‡i da ne moÅ¾e linearno odvojiti...
TakoÄ‘er primjeÄ‡ujem da se Å¡irina margine poveÄ‡ala na otprilike 1.5.


################## 2 ##################

Å to je C veÄ‡i to je margina tvrÄ‘a, tj viÅ¡e kaÅ¾njava primjere unutar margine.
Pa Å¡to je C veÄ‡i model je sloÅ¾eniji. A gamma Å¡to je veÄ‡a primjeri su meÄ‘usobno 
razliÄitiji, pa to moÅ¾e dovesti do prenauÄenosti.
Å to je preciznost(gamma) veÄ‡a, C treba biti manji, tj. treba jaÄe
regularizirati model.


################## 3 ##################
Q: Razlikuje li se povrÅ¡ina pogreÅ¡ke na skupu za uÄenje i skupu za ispitivanje?
ZaÅ¡to?

A: U primjeru sa znaÄajkama u 2 dimenzije. Test error je sliÄan train erroru,
ali svejedno malo viÅ¡i. (Svijetlija podruÄja predstavljaju maju greÅ¡ku). Dakle,
model dobro generalizira, odnosno optimizacija rezultira hiperparametrima koji
dobro predviÄ‘aju na test setu.
U primjeru sa znaÄajkama u 100 dimenzija, test error je puno veÄ‡i od train
errora. Mijenjanje gamme u test erroru baÅ¡ i ne utjeÄe previÅ¡e na poboljÅ¡anje 
toÄnosti. Mislim da je to zbog toga Å¡to primjeri imaju 100 znaÄajki i samim 
time su manje sliÄniji, joÅ¡ kada poveÄ‡amo preciznost ni jedan primjer si neÄ‡e
biti sliÄan. Jedino Å¡to pomaÅ¾e je C. Na train setu taj C Å¡to je veÄ‡i trebala
bi biti manja pogreÅ¡ka. Ali naravno da Å¡to je veÄ‡i C, model je sloÅ¾eniji, pa
Ä‡e loÅ¡e predviÄ‘ati na test setu. **Valda**


Q: U prikazu povrÅ¡ine pogreÅ¡ke, koji dio povrÅ¡ine odgovara prenauÄenosti, a koji
podnauÄenosti? ZaÅ¡to?

A: Mislim da tamo di je najsvijetlije na test setu oznaÄava di je optimum modela,
a kod test seta dole lijevo, gdje je najmanji C i najmanji gamma tamo je 
podnauÄenost, a tamo di je najveÄ‡i C i najveÄ‡i gamma prenauÄenost.


Q: Kako broj dimenzija ğ‘› utjeÄe na povrÅ¡inu pogreÅ¡ke, odnosno na optimalne 
hiperparametre (ğ¶âˆ—,ğ›¾âˆ—)?

A: U prvom primjeru opt_C = 1, opt_gamma = 8, dok u drugom primjeru sa 100 
znaÄajki  opt_C = 2, opt_gamma = 0.000488. Iz toga moÅ¾emo vidjeti da se gamma
smanjuje sa veliÄinom dimenzije, a C se malo poveÄ‡ao.



Q: Preporuka je da poveÄ‡anje vrijednosti za ğ›¾ treba biti popraÄ‡eno smanjenjem 
vrijednosti za ğ¶. Govore li vaÅ¡i rezultati u prilog toj preporuci? ObrazloÅ¾ite. 

A: Pa to se moÅ¾e vidjeti iz optimalnih vrijednosti tih parametara navednih u
proÅ¡lom odgovoru. Å to je veÄ‡i C to je manji gamma i obrnuto.



################## 4 ##################
b)
Q: Kako radi ovo skaliranje?
A: RasporeÄ‘uje vrijednosti u intervalu [0,1]


Q: Dobiveni histogrami su vrlo sliÄni. U Äemu je razlika? 
A: Neskalirane vrijednosti i minMax skalirane vrijednosti izgledaju isto,
ali zapravo vrijednosti kod minMax skaliranja su rasporeÄ‘ene u intervalu [0,1].
Sami odnos tih vrijednosti se nije zapravo promijenio pa grafovi izgledaju isto
na prvu.


c)
Q: Kako radi ovo skaliranje?
A: Centrira vrijednosti oko medijana, a njihov otklon od tog medijana jednak 
je iznosu standardne devijacije.


Q: Dobiveni histogrami su vrlo sliÄni. U Äemu je razlika? 
A: Ne vidim baÅ¡ kako su sliÄni...

d)
Q: Jesu li rezultati oÄekivani? ObrazloÅ¾ite.
A: OÄekivao sam da Ä‡e standardno skaliranje biti najbolje, Å¡to se i pokazalo.
Ali zaÅ¡to je minMax gori od neskaliranih podataka nisam siguran. MoÅ¾da Å¡to su
i dalje jedne znaÄajke lijevo, a druge desno kao i kod neskaliranih, samo 
Å¡to ovaj put su na manjem intervalu, pa se moÅ¾da izgubila preciznost.




Q: Bi li bilo dobro kada bismo funkciju fit_transform primijenili na 
cijelom skupu podataka? ZaÅ¡to?
A: Mislim da nema smisla na svim podacima koristiti fit_transform, buduÄ‡i
da model treniramo samo na train podacima, pa nam je korisnije koristiti
samo to skaliranje u odnosu na train set.


Q: Bi li bilo dobro kada bismo tu funkciju primijenili zasebno na skupu za
uÄenje i zasebno na skupu za ispitivanje? ZaÅ¡to?
A: Opet mislim da ne. Ako se drÅ¾imo toga da iz train seta uÄimo, onda istu 
skalu koju imamo na train setu trebamo koristiti i na test setu. 


################## 5 ##################

U fit metodi, zapravo se ne odvija nekakvo uÄenje nego samo postavljamo te 
"susjede". 
I onda predikcija se odvija na naÄin da gledamo koje su oznake najbliÅ¾ih k susjeda.



################## 6 ##################
a)
Q: Kako ğ‘˜ utjeÄe na izgled granice izmeÄ‘u klasa?
A: Kad je k=1 dobijemo viÅ¡e izdvojenih podruÄja. Dok sa veÄ‡im k lijeva strana 
veÄ‡inom pripada jednoj klasi . MoÅ¾emo reÄ‡i da je granica glaÄ‘a


Q: Kako se algoritam ponaÅ¡a u ekstremnim situacijama: ğ‘˜=1 i ğ‘˜=100?
A: Kad je k=1 model je prenauÄen jer gleda samo jednog najbliÅ¾eg susjeda,
a kada je k=100 model je prejednostavan, jer u ovom sluÄaju pogleda sve primjere
jer ih je ukupno 100.


b)
Q: Kako se mijenja optimalna vrijednost hiperparametra ğ‘˜ s obzirom na broj 
primjera ğ‘? ZaÅ¡to?
A: Raste. Pa moÅ¾da, zato Å¡to ako imamo viÅ¡e primjera (N) model je sloÅ¾eniji, 
jer samim time ima i viÅ¡e parametara. Pa ga Å¾elimo pojednostaviti gledajuÄ‡i
veÄ‡i broj susjeda.


Q: Kojem podruÄju odgovara prenauÄenost, a kojem podnauÄenost modela? 
ZaÅ¡to?
A: Vidi se na slici. U test erroru.



Q: Je li uvijek moguÄ‡e doseÄ‡i pogreÅ¡ku od 0 na skupu za uÄenje?
A: Da kada je k=1, onda je sliÄnost podatka iz skupa na uÄenje najbliÅ¾i samom
sebi i automatski uzima njegovu oznaku.



c)
Q: Je li algoritam k-najbliÅ¾ih susjeda osjetljiv na nebitne znaÄajke? 
ZaÅ¡to?
A: Je, greÅ¡ka mu je veÄ‡a sa nebitnim znaÄajkama. Pa znaÄajke koje su nam nebitne
svejedno sudjeluju u predikciji, tj. njih takoÄ‘er usporeÄ‘ujemo sa drugim nebitnim
znaÄajkama i onda nam predikcija ovisi o tim nebitnim znaÄajkama i samim time je 
manja preciznost.

Q: Je li ovaj problem izraÅ¾en i kod ostalih modela koje smo dosad 
radili (npr. logistiÄka regresija)?
A: Je. Zbog nebitnih znaÄajki dobivamo bespotrebno sloÅ¾enije modele, koji Äine
veÄ‡u greÅ¡ku na neviÄ‘enim primjerima.



Q: Kako bi se model k-najbliÅ¾ih susjeda ponaÅ¡ao na skupu podataka sa 
znaÄajkama razliÄitih skala? Detaljno pojasnite.
A: Ako je veÄ‡a skala neke znaÄajke. Samim time bit Ä‡e i veÄ‡a npr euklidska 
udaljenost, pa Ä‡e primjeri bit viÅ¡e udaljeni samo ako im se ta znaÄajka sa 
velikom skalom Äak i malo razlikuje.



################## 7 ##################
Q: PokuÅ¡ajte objasniti razlike u rezultatima. Koju biste od ovih dviju 
mjera koristili za klasifikaciju visokodimenzijskih podataka?
A: Euklidska udaljenost se poveÄ‡eva poveÄ‡avanjem udaljenosti izmeÄ‘u
primjera, samim time primjeri su udaljeniji i time se gube razlike izmeÄ‘u
primjera. Dok kod kosinusna udaljenost predstavlja kosinus kuta izmeÄ‘u 
dvije toÄke, Å¡to Ä‡e biti ograniÄena vrijednost , a i takoÄ‘er kut ne raste zbog
poveÄ‡ane udaljenosti izmeÄ‘u toÄaka. Koristio bi kosinusnu udaljenost jer ne
raste s poveÄ‡anjem dimenzionalnosti.


Q: ZaÅ¡to je ovaj problem osobito izraÅ¾en kod algoritma k-najbliÅ¾ih susjeda?
A: Pa valjda zato Å¡to se u tom algoritmu koristi sliÄnost za predikciju,
a Äesto se koristi euklidska udaljenost.Vrijednosti jezgrenih funkcija za sve 
parove primjera iz skupa za uÄenje D moÅ¾emo izraÄunati